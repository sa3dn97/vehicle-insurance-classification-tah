# -*- coding: utf-8 -*-
"""VIC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GvMezF4kut86hLpRfRELf-zcFyb9e_po

### Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.combine import SMOTETomek, SMOTEENN
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint as sp_randint
from scipy.stats import uniform as sp_uniform
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Models:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
import lightgbm

# Evaluation
from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score

"""### Read The Data"""

try:
  df = pd.read_csv('Data/train_set.csv')
  test = pd.read_csv('Data/test_set.csv')
  sample_sub = pd.read_csv('Data/sub_example.csv')
except Exception:
  print('No such file or directory...')

df.head()

test.head()

"""### Data Analysis"""

print('Train shape: ', df.shape)
print('Test shape: ', test.shape)

df.dtypes.value_counts()

print('Object columns: \n', df.select_dtypes(include='object').columns.tolist())
print('Int64 columns: \n', df.select_dtypes(include='int64').columns.tolist())
print('Float64 columns: \n', df.select_dtypes(include='float64').columns.tolist())

df1 = df.drop('id',axis=1)

df1.isnull().sum()

df1.describe().T

df1.describe(include='O')

df1.info()

df1.drop(['Gender', 'Vehicle_Age', 'Vehicle_Damage'], axis=1).hist(figsize=(30, 25), grid=True)

driving = df1[['Driving_License', 'Response']]
print('for Driving_License = 0:\n\n', driving[driving['Driving_License']==0]['Response'].value_counts())
print('=============================\nfor Driving_License = 1:\n\n', driving[driving['Driving_License']==1]['Response'].value_counts())

# df1 = df1.drop('Driving_License', axis=1)

fig, ax = plt.subplots(4,2, figsize=(15,15))
explode = (0, 0.1, 0, 0)
for count, value in enumerate(['Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Previously_Insured']):
    freq_table = pd.crosstab(index=df1[value], columns='frequancy')
    # visualize bar_plot
    ax[count, 0].bar(freq_table.index.tolist(), freq_table['frequancy'],  width=0.4)
    ax[count, 0].grid()
    ax[count, 0].set_title(str(value)+' frequancy')
    ax[count, 0].set_ylabel('frequancy')
    # visualize pie_plot
    sizes = np.array(freq_table['frequancy'].values.tolist())
    sizes = (sizes/len(df1))*100
    max_val = max(sizes)
    max_index = np.where(sizes == max_val)
    explode = tuple([0 if i!=max_index[0][0] else 0.1 for i in range(len(sizes))])
    ax[count, 1].pie(sizes, explode=explode, labels=None, pctdistance=1.2, autopct='%1.1f%%', shadow=True, startangle=90)
    ax[count, 1].legend(labels=freq_table['frequancy'].index, loc=3)
    ax[count, 1].axis('equal')

data = df1.copy()

data['Region_Code'] =data['Region_Code'].astype('int64')
data['Policy_Sales_Channel'] =data['Policy_Sales_Channel'].astype('int64')

fig, ax = plt.subplots(5,2, figsize=(15,17))
for count1, value in enumerate(['Age', 'Region_Code', 'Policy_Sales_Channel', 'Annual_Premium', 'Vintage']):
    # visulaize box plot
    sns.boxplot(data=data, x=data[value], ax=ax[count1, 0])
    ax[count1, 0].set_ylabel(str(value))
    ax[count1, 0].grid()
    # visulaize histogram plot
    bins = len(data[value].unique())//2
    sns.histplot(data=data, x=data[value], bins=20, kde=True, ax=ax[count1, 1])

print('The number of Vintage unique value: ', len(data.Vintage.unique()))
print(data.Vintage.value_counts())

"""### Remove the outliers using IQR"""

Q1 = data.Annual_Premium.quantile(0.25)
Q3 = data.Annual_Premium.quantile(0.75)
IQR = Q3 - Q1
maxi = (1.5 * IQR) + Q3
print('The number of outliers in Annaul_Premium: ', data[data.Annual_Premium > maxi].shape[0])

continuous_features = ['Age', 'Region_Code', 'Policy_Sales_Channel', 'Vintage', 'Annual_Premium']
def detect_outLier_features(df):
    for feature in continuous_features:
        q1 = np.quantile(df[feature], 0.25, interpolation='midpoint')
        q3 = np.quantile(df[feature], 0.75, interpolation='midpoint')
        IQR = q3 - q1
        upper_outlier = q3 + (1.5 * IQR)
        lower_outlier = q1 - (1.5 * IQR)
        print(str(feature)+' IQR :', IQR)
        print(str(feature)+' Upper Outlier :', upper_outlier)
        print(str(feature)+' Lower Outlier :', lower_outlier)
        print('==============================================')
detect_outLier_features(data)

cols = list(data[continuous_features])
Q1 = data[cols].quantile(0.25)
Q3 = data[cols].quantile(0.75)
IQR = Q3 - Q1
condition = ~((data[cols] < (Q1 - 1.5 * IQR)) | (data[cols] > (Q3 + 1.5 * IQR))).any(axis=1)
data = data[condition]

data.shape

fig, ax = plt.subplots(1,2, figsize=(15,3))
# visulaize box plot
sns.boxplot(data=data, x=data.Annual_Premium, ax=ax[0])
ax[0].set_ylabel('Annual Premium')
ax[0].grid()
# visulaize histogram plot
bins = len(data.Annual_Premium.unique())//2
sns.histplot(data=data, x=data.Annual_Premium, bins=20, kde=True, ax=ax[1])

# data = data[data.Annual_Premium>7000]
# fig, ax = plt.subplots(1,2, figsize=(15,3))
# # visulaize box plot
# sns.boxplot(data=data, x=data.Annual_Premium, ax=ax[0])
# ax[0].set_ylabel('Annual Premium')
# ax[0].grid()
# # visulaize histogram plot
# bins = len(data.Annual_Premium.unique())//2
# sns.histplot(data=data, x=data.Annual_Premium, bins=20, kde=True, ax=ax[1])

print(data.shape)
data.isnull().sum()

Target = data['Response']
data = data.drop('Response',axis=1)

"""### Encoder"""

## male => 0 
## Female => 1 
data['Gender'] = data['Gender'].map({'Male': 0,'Female': 1})

# No => 0
# Yes => 1
data['Vehicle_Damage'] = data['Vehicle_Damage'].map({'No': 0,'Yes': 1})

# < 1 Year => 1
# 1-2 Year => 2
# > 2 Years => 3 
data['Vehicle_Age'] = data['Vehicle_Age'].map({'< 1 Year': 1,'1-2 Year': 2,'> 2 Years':3})

data.info()

discrete_features = ['Gender', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage']

"""### Correlations"""

Target.value_counts()

print('The Target Variable')
fig, ax = plt.subplots(1,2,figsize=(12,5))
freq_table = pd.crosstab(index=Target, columns='frequancy')
# visualize bar_plot
ax[0].bar(freq_table.index.tolist(), freq_table['frequancy'],  width=0.4)
ax[0].grid()
# visualize pie_plot
sizes = np.array(freq_table['frequancy'].values.tolist())
sizes = (sizes/len(Target))*100
max_val = max(sizes)
max_index = np.where(sizes == max_val)
explode = tuple([0 if i!=max_index[0][0] else 0.1 for i in range(len(sizes))])
ax[1].pie(sizes, explode=explode, labels=None, pctdistance=1.2, autopct='%1.1f%%', shadow=True, startangle=90)
ax[1].legend(labels=freq_table['frequancy'].index, loc=3)
ax[1].axis('equal')

fig, ax = plt.subplots(figsize=(10,10))
corr = pd.concat([data, Target], axis=1).corr()
matrix = np.triu(corr)
sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns,annot=True, cmap="BuPu", mask=matrix)

plt.show()

weights = {0:0.162, 1:0.838}

# smt=SMOTETomek(sampling_strategy='auto')
# smt.fit(data, Target)
# X_resampled, y_resampled = smt.fit_resample(data, Target)

# smt=SMOTEENN(random_state=42)
# smt.fit(data, Target)
# X_resampled, y_resampled = smt.fit_resample(data, Target)

# print('The Target after Resampling: ', y_resampled.shape)
# print('The Features after Resampling: ', X_resampled.shape)

# print('The Target Variable')
# fig, ax = plt.subplots(1,2,figsize=(12,5))
# freq_table = pd.crosstab(index=y_resampled, columns='frequancy')
# # visualize bar_plot
# ax[0].bar(freq_table.index.tolist(), freq_table['frequancy'],  width=0.4)
# ax[0].grid()
# # visualize pie_plot
# sizes = np.array(freq_table['frequancy'].values.tolist())
# sizes = (sizes/len(Target))*100
# max_val = max(sizes)
# max_index = np.where(sizes == max_val)
# explode = tuple([0 if i!=max_index[0][0] else 0.1 for i in range(len(sizes))])
# ax[1].pie(sizes, explode=explode, labels=None, pctdistance=1.2, autopct='%1.1f%%', shadow=True, startangle=90)
# ax[1].legend(labels=freq_table['frequancy'].index, loc=3)
# ax[1].axis('equal')

# fig, ax = plt.subplots(figsize=(10,10))
# corr = pd.concat([y_resampled, X_resampled], axis=1).corr()
# matrix = np.triu(corr)
# sns.heatmap(corr,
#         xticklabels=corr.columns,
#         yticklabels=corr.columns,annot=True, cmap="BuPu", mask=matrix)
# plt.show()

# X_resampled

X_resampled =data.drop(['Vintage','Region_Code', 'Previously_Insured', 'Driving_License', 'Annual_Premium'],axis=1)

X_resampled.shape

"""### Scaling"""

# scale = RobustScaler().fit(X_resampled)
# features = scale.transform(X_resampled)
# features

"""### Split and test models"""

train, valid, target, valid_target = train_test_split(X_resampled, Target, test_size = 0.25, random_state=42)

def classi_repo(valid_target, pred, model_name):
    print(model_name+':')
    print('F1-Score: ', f1_score(valid_target, pred), '\n\n')
    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,5))
    clf_report = classification_report(valid_target,
                                   pred,
                                   output_dict=True)
    g = sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap='Blues', ax=ax1)
    g.set_title('Classification Report\n\n');
    #=========================================
    cf_matrix = confusion_matrix(valid_target, pred)
    g1 = sns.heatmap(cf_matrix, annot=True, cmap='Blues',ax=ax2)
    g1.set_title('Confusion Matrix\n\n');
    g1.set_xlabel('\nPredicted Values')
    g1.set_ylabel('Actual Values ');
    g1.xaxis.set_ticklabels(['False','True'])
    g1.yaxis.set_ticklabels(['False','True'])
    
    plt.show()

"""#### Random Forest Classifier"""

ran = RandomForestClassifier(class_weight=weights)
ran.fit(train, target)
ran_pre = ran.predict(valid)
print('Random Forest predictions: ', ran_pre)

classi_repo(valid_target, ran_pre, 'Random Forest Classifier')

"""#### LightGBM Classifier"""

lgbm = lightgbm.LGBMClassifier(random_state=42, class_weight='balanced')
lgbm.fit(train, target)
lgbm_pre = lgbm.predict(valid)
print('LightGBM predictions: ', lgbm_pre)

classi_repo(valid_target, lgbm_pre, 'LightGBM Classifier')

"""#### AdaBoost Classifier"""

clf = AdaBoostClassifier(random_state=42)
clf.fit(train, target)
clf_pre = clf.predict(valid)
print('AdaBoost predictions: ', clf_pre)

classi_repo(valid_target, clf_pre, 'AdaBoost Classifier')

"""### Tuning The Model & Resampling

#### LightGBM Classifier
"""

# features = pd.DataFrame(features)
# f1_list = []
# param_test ={'num_leaves': sp_randint(6, 50), 
#              'min_child_samples': sp_randint(100, 500), 
#              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],
#              'subsample': sp_uniform(loc=0.2, scale=0.8), 
#              'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),
#              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],
#              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}
# clf = lightgbm.LGBMClassifier(random_state=42)
# fold = KFold(n_splits=5, shuffle=True, random_state=42)
# gs = RandomizedSearchCV(
#     estimator=clf, param_distributions=param_test, 
#     n_iter=50,
#     scoring='f1',
#     cv=3,
#     random_state=42,
#     verbose=2, n_jobs = -1)
# for count, (train_index, val_index) in enumerate(fold.split(X=features, y=Target)):
#     print('Fold number ', count, ': ')
#     x_train, x_val = features.iloc[train_index], features.iloc[val_index]
#     y_train, y_val = Target.iloc[train_index], Target.iloc[val_index]
#     pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), gs)
#     model = pipeline.fit(x_train, y_train)
#     best_est = gs.best_estimator_
#     pred = best_est.predict(x_val)
#     f1_list.append(f1_score(y_val, pred))
# print("f1: {}".format(np.mean(f1_list)))

"""#### Random Forest Classifier"""

features = X_resampled.copy()
f1_list = []
roc_auc = []
n_estimators = [100]
max_features = ['auto', 'sqrt']
max_depth = [int(x) for x in np.linspace(10, 60, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]
param_test ={'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
fold = KFold(n_splits=3, shuffle=True, random_state=42)
for count, (train_index, val_index) in enumerate(fold.split(X=features, y=Target)):
    print('Fold number ', count, ': ')
    x_train, x_val = features.iloc[train_index], features.iloc[val_index]
    y_train, y_val = Target.iloc[train_index], Target.iloc[val_index]
    
    x_resampled ,y_resampled = SMOTE(sampling_strategy='minority').fit_resample(x_train, y_train)
    
    scale_train = RobustScaler().fit_transform(x_resampled)
    scale_train = pd.DataFrame(scale_train)
    clf = RandomForestClassifier(random_state=42)
    gs = RandomizedSearchCV(
    estimator=clf, param_distributions=param_test, 
    n_iter=30,
    cv=3,
    random_state=42,
    verbose=2, n_jobs = -1).fit(scale_train, y_resampled)
    
    scale_val = RobustScaler().fit_transform(x_val)
    
    # pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'),gs)
    # model = pipeline.fit(x_train, y_train)
    
    best_est = gs.best_estimator_
    pred = best_est.predict(scale_val)
    f1_list.append(f1_score(y_val, pred))
    roc_auc.append(roc_auc_score(y_val, pred))
print("f1: {}".format(np.mean(f1_list)))
print("auc: {}".format(np.mean(roc_auc)))

"""#### AdaBoost Classifier"""

# features = pd.DataFrame(features)
# f1_list = []
# param_test ={'n_estimators': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 20],
#     'learning_rate': [(0.97 + x / 100) for x in range(0, 8)],
#     'algorithm': ['SAMME', 'SAMME.R']}
# clf = AdaBoostClassifier(random_state=42)
# fold = KFold(n_splits=5, shuffle=True, random_state=42)
# gs = RandomizedSearchCV(
#     estimator=clf, param_distributions=param_test, 
#     n_iter=50,
#     scoring='f1',
#     cv=3,
#     random_state=42,
#     verbose=2, n_jobs = -1)
# for count, (train_index, val_index) in enumerate(fold.split(X=features, y=Target)):
#     print('Fold number ', count, ': ')
#     x_train, x_val = features.iloc[train_index], features.iloc[val_index]
#     y_train, y_val = Target.iloc[train_index], Target.iloc[val_index]
#     pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), gs)
#     model = pipeline.fit(x_train, y_train)
#     best_est = gs.best_estimator_
#     pred = best_est.predict(x_val)
#     f1_list.append(f1_score(y_val, pred))
# print("f1: {}".format(np.mean(f1_list)))

"""### Test Data"""

test.head()

test.isnull().sum()

test.drop(['id','Vintage', 'Region_Code', 'Previously_Insured', 'Driving_License', 'Annual_Premium'], axis=1, inplace=True)

test.shape

test['Vehicle_Age'] = test['Vehicle_Age'].map({'< 1 Year': 1,'1-2 Year': 2,'> 2 Years':3})
test['Gender'] = test['Gender'].map({'Male': 0,'Female': 1})

test['Vehicle_Damage'] = test['Vehicle_Damage'].map({'No': 0,'Yes': 1})

scale = RobustScaler().fit(test)
test_df = scale.transform(test)
test_df

test_pre = model.predict(test_df)

test_pre[:10]

print('The number of 1: ', len(test_pre[test_pre==1]))
print('The number of 0: ', len(test_pre[test_pre==0]))

"""### CSV Result"""

sample_sub['Response'] = test_pre

sample_sub.to_csv('sub4.csv', index=False)

